{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hsj9ccQkLxs",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# Get data\n",
        "import requests \n",
        "\n",
        "fetch = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
        "\n",
        "# removing first 553 characters (filler text)\n",
        "data = fetch.text[553:5757527]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5KNtAQ-obxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn data into integers rather than raw text\n",
        "\n",
        "# unique chars \n",
        "chars = list(set(data))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW8yb-Tho7L1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4832ebe-b93f-4e8e-a46d-5c6a5626be52"
      },
      "source": [
        "# how many chars in data?\n",
        "len(chars)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1LBMDWTpJsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1a7f1af-5332-4eb5-f8ca-51d77e02168b"
      },
      "source": [
        "# encode data using char_int\n",
        "encoded = [char_int[c] for c in data]\n",
        "len(encoded) == len(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpGoUT_Ik0RB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f640d3a5-b2cf-4cb5-9dae-ae4db9e81782"
      },
      "source": [
        "# reshape into useable text sequences\n",
        "sequences = []\n",
        "next_char = []\n",
        "maxlen = 60\n",
        "step = 20\n",
        "\n",
        "for i in range(0, len(encoded)-maxlen, step):\n",
        "  curr_sequence = encoded[i:i+maxlen]\n",
        "  sequences.append(curr_sequence)\n",
        "  next_char.append(encoded[maxlen+i])\n",
        "\n",
        "len(sequences), len(next_char)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(287846, 287846)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9QNvFHcl23f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b3882ec-6995-4b85-b37d-804f795b2154"
      },
      "source": [
        "# make sure a sequence looks as expected\n",
        "sequences[0], next_char[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([59,\n",
              "  26,\n",
              "  89,\n",
              "  105,\n",
              "  103,\n",
              "  77,\n",
              "  54,\n",
              "  59,\n",
              "  23,\n",
              "  103,\n",
              "  54,\n",
              "  20,\n",
              "  95,\n",
              "  0,\n",
              "  5,\n",
              "  105,\n",
              "  103,\n",
              "  89,\n",
              "  103,\n",
              "  54,\n",
              "  69,\n",
              "  95,\n",
              "  16,\n",
              "  4,\n",
              "  68,\n",
              "  54,\n",
              "  95,\n",
              "  80,\n",
              "  54,\n",
              "  69,\n",
              "  26,\n",
              "  105,\n",
              "  105,\n",
              "  26,\n",
              "  13,\n",
              "  0,\n",
              "  54,\n",
              "  46,\n",
              "  23,\n",
              "  13,\n",
              "  4,\n",
              "  103,\n",
              "  68,\n",
              "  5,\n",
              "  103,\n",
              "  13,\n",
              "  16,\n",
              "  103,\n",
              "  2,\n",
              "  90,\n",
              "  2,\n",
              "  90,\n",
              "  47,\n",
              "  27,\n",
              "  89,\n",
              "  23,\n",
              "  95,\n",
              "  16,\n",
              "  77,\n",
              "  54],\n",
              " 69)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAfbKQmHphhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9451f98-d5f5-40b8-92bd-92e332026d86"
      },
      "source": [
        "# shape data into final format\n",
        "import numpy as np \n",
        "\n",
        "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "  for j, char in enumerate(sequence):\n",
        "    X[i, j, char] = 1\n",
        "  y[i, next_char[i]] = 1\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((287846, 60, 107), (287846, 107))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb-Tz0Lwsz20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f92833f6-8c60-4189-c187-1eb2bae6576a"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc8e8ADis-9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "2fdc190e-ef76-4ae3-aae3-b7edca9e8d8b"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer='adam', \n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               120832    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 107)               13803     \n",
            "=================================================================\n",
            "Total params: 134,635\n",
            "Trainable params: 134,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEGBTIttp4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "import random \n",
        "import sys \n",
        "\n",
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = data[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        # sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKdQSE8fuhN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0247dd75-cd31-48b7-b8a0-c5e05cbf45a6"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(X, y,\n",
        "          batch_size=1028,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 287846 samples\n",
            "Epoch 1/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.9166 - accuracy: 0.4430\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"int.\n",
            "  AUMERLE. Thou dar'st not, coward, live to see that d\"\n",
            "int.\n",
            "  AUMERLE. Thou dar'st not, coward, live to see that dround.\n",
            "  CEROID DON ILY Fras!\n",
            "\n",
            " Hene lortne;\n",
            "What so is neane dien'd. For'l now is andty, whece hericfur:\n",
            "    eake dorly the puct anat to ancouefay.\n",
            "  CAMETA.\n",
            "\n",
            "\n",
            " [_eriny bus itang her didtonch.\n",
            "\n",
            "\n",
            "\n",
            " BOVERE\n",
            "ON Cuce of, whan ith, and. GUUREs EDRVINT, OwLecA. SlEme foou, wy, Moftr, ghe praseen speno, grie,\n",
            "Yo\n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.9167 - accuracy: 0.4430\n",
            "Epoch 2/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8995 - accuracy: 0.4479\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"he make this way,\n",
            "    Under the colour of his usual game,\n",
            "\"\n",
            "he make this way,\n",
            "    Under the colour of his usual game,\n",
            "\n",
            "Catsead a the ingate whas hom hertenr, in. sowl sorns prong theeyâ.\n",
            "\n",
            "[_Brave, in fold.\n",
            "Theal spee duss the wrold tay, fir ad acf\n",
            "    BeAng thuth a im heme laum thiout hat\n",
            "    And so not se prawied thy rozenceny well tan eave\n",
            "    I he ranetor hed we dofrs wheas seoul her moke thus quaing,\n",
            "And bope with mounatt thre! hiy I mart\n",
            "    I \n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.8992 - accuracy: 0.4479\n",
            "Epoch 3/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8835 - accuracy: 0.4514\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"nk.\n",
            "\n",
            "\n",
            "      FRIAR.\n",
            "      To do what, signior?\n",
            "\n",
            "\n",
            "     \"\n",
            "nk.\n",
            "\n",
            "\n",
            "      FRIAR.\n",
            "      To do what, signior?\n",
            "\n",
            "\n",
            "                                TIUSA ENwanc: JANC, RWC_Krin, Say\n",
            "  TAYDAES. And you troogh.\n",
            "  BONANIHe sartutes, the come to dode you, or chall comats; far?\n",
            "  RORDHONDY. You my low, in thear HAndsecowio hame fursex'd;\n",
            "    In beang und your mess. Has, with Pathrounw ith my benosores, and no vage.\n",
            "\n",
            "HRY. That not know woll not or maer for;\n",
            "Thert hesh mar, stay to hise the grigh hime,\n",
            "Make to see \n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.8834 - accuracy: 0.4514\n",
            "Epoch 4/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8667 - accuracy: 0.4556\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"and for the liberal arts,\n",
            "Without a parallel: those being a\"\n",
            "and for the liberal arts,\n",
            "\n",
            "Wath yourd âe rastper well hasfucherker.\n",
            "\n",
            "\n",
            "O Heysturyrtore,\n",
            "For war ore of houter: betrest you malf is ceare\n",
            "Ande stronothess cateer? Permen thi'e in.\n",
            "Wer his with arin bofion wor thour cope,\n",
            "God tistated!\n",
            "\n",
            " BARIAL.\n",
            "\n",
            "  QUELES. Yow ail venkior; and you pureof or cogiond\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.8668 - accuracy: 0.4555\n",
            "Epoch 5/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8523 - accuracy: 0.4584\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"n\n",
            "After his studies or his usual pain?\n",
            "Then give me leave \"\n",
            "n\n",
            "After his studies or his usual pain?\n",
            "Then give me leave Lodes ginsung is he hil.\n",
            "A The rein ruspedst been to mighâd a live dot dear.\n",
            "\n",
            "    Which leve ama gorse the feilicr troust conge tleepâ to mus deow,\n",
            "\n",
            "  O I LuCt PeO, and Lorce; pirk-\n",
            "         \"  30\n",
            "  THIND. Sy domg. But be.\n",
            "  WASTOR. O goon have witha det hy\n",
            "   [Th o sean, with prome nance acd take ffor whet, I is,\n",
            "    Whe quese is these be ch\n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.8525 - accuracy: 0.4584\n",
            "Epoch 6/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8368 - accuracy: 0.4623\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"    A B C; to weep, like a young wench that had buried her g\"\n",
            "    A B C; to weep, like a young wench that had buried her gres;, dpouble bises?\n",
            "    And no be jundes nesour har'dst bearn, of bedy my fagcues mis ertior,\n",
            "Bet fill rear? Ang if!\n",
            "\n",
            "_Mleelien the sarmiatmon neetsedsard whatio,\n",
            "     lispy liow I bavy a but tiethât dist!\n",
            "    I' prall mam of thou sties ow gockes! Nomma\n",
            "    Is learâd you bunce, dut nom wills tonm toosert;\n",
            "'spats ane I aw thee Litca, the grittant of oub.\n",
            "\n",
            "DOMBUD.\n",
            "Cousnt on Coles wen\n",
            "287846/287846 [==============================] - 19s 64us/sample - loss: 1.8369 - accuracy: 0.4623\n",
            "Epoch 7/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8247 - accuracy: 0.4656\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"y, which is no grief to give.\n",
            "  GLOUCESTER. A greater gift \"\n",
            "y, which is no grief to give.\n",
            "\n",
            "    Thy undusherrce that or weach Loth, Crepirg. A Conithas,\n",
            "\n",
            "       if so hiliave it you hand the vorret.\n",
            "  USILDAf. I'r bo is with ill, bloosh of on, that be?\n",
            "\n",
            "\n",
            "    Bud your tomen this pad trou\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.8246 - accuracy: 0.4656\n",
            "Epoch 8/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8132 - accuracy: 0.4689\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"; they lack retention.\n",
            "Alas, their love may be called appet\"\n",
            "; they lack retention.\n",
            "Alas, their love may be called appetcent\n",
            "partlering to girest beat by tropp would eastles mane, is in hourseld,\n",
            "in oulr is nove joane faim, in my coust.\n",
            "\n",
            "AOTIANT.\n",
            "Iâlot covellostâs our wherield deater Beaagst,\n",
            "\n",
            "\n",
            "It uncestrek.\n",
            "\n",
            "\n",
            "I to, (ave ily?\n",
            "\n",
            "\n",
            "Entir forenterat and lave of that in will it in theâ\n",
            "Whan thou tray print Biy fitit\n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.8131 - accuracy: 0.4689\n",
            "Epoch 9/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8027 - accuracy: 0.4715\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"ll?\n",
            "\n",
            "IACHIMO.\n",
            "Thanks, madam; well. Beseech you, sir,\n",
            "Des\"\n",
            "ll?\n",
            "\n",
            "IACHIMO.\n",
            "Thanks, madam; well. Beseech you, sir,\n",
            "\n",
            "\n",
            "A mise as froulss dost or nomen, of mike an the\n",
            "Obereâd friee you puy, sme haveringld,\n",
            "Even time your houlfâd, I pail hever he\n",
            "  cering love not hear, as moraty With'r Sake loun;\n",
            "The gade; pealfoner the rordpod me.\n",
            "\n",
            "Fould \n",
            "amas is dy Mastinim!\n",
            "Thene enmand\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 1.8028 - accuracy: 0.4715\n",
            "Epoch 10/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.7887 - accuracy: 0.4747\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"therefore, to revenge it, shalt thou die;\n",
            "    And so should\"\n",
            "therefore, to revenge it, shalt thou die;\n",
            "\n",
            "    That for slaer Freacuid herefunien lort\n",
            "\n",
            "\n",
            "    Sould you ponerstal to. Compor Goffeforâd,\n",
            "    For thene kingh will fell it to fe,\n",
            "    Is yousfouss wishtigsted shaw is bodre\n",
            "All wree, amens, And bestyel; fithole are the hair ablids; thot opest,\n",
            "\n",
            "\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.7888 - accuracy: 0.4746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f32c55f7e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}