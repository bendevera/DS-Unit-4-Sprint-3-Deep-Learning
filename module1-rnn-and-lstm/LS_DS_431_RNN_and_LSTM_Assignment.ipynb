{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hsj9ccQkLxs",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# Get data\n",
        "import requests \n",
        "\n",
        "fetch = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
        "\n",
        "# removing first 553 characters (filler text)\n",
        "data = fetch.text[553:5757527]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5KNtAQ-obxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn data into integers rather than raw text\n",
        "\n",
        "# unique chars \n",
        "chars = list(set(data))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW8yb-Tho7L1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e075406b-a8b3-4286-d7f9-b4f9f5ad01fe"
      },
      "source": [
        "# how many chars in data?\n",
        "len(chars)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1LBMDWTpJsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edba81f4-358d-46dd-be38-7eda55ed4394"
      },
      "source": [
        "# encode data using char_int\n",
        "encoded = [char_int[c] for c in data]\n",
        "len(encoded) == len(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpGoUT_Ik0RB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42e58a92-6b93-491f-bb04-19f704fc9890"
      },
      "source": [
        "# reshape into useable text sequences\n",
        "sequences = []\n",
        "next_char = []\n",
        "maxlen = 60\n",
        "step = 20\n",
        "\n",
        "for i in range(0, len(encoded)-maxlen, step):\n",
        "  curr_sequence = encoded[i:i+maxlen]\n",
        "  sequences.append(curr_sequence)\n",
        "  next_char.append(encoded[maxlen+i])\n",
        "\n",
        "len(sequences), len(next_char)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(287846, 287846)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9QNvFHcl23f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41b15e74-bce9-4080-c8f0-b8599ada1516"
      },
      "source": [
        "# make sure a sequence looks as expected\n",
        "sequences[0], next_char[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2,\n",
              "  15,\n",
              "  72,\n",
              "  80,\n",
              "  31,\n",
              "  79,\n",
              "  27,\n",
              "  2,\n",
              "  26,\n",
              "  31,\n",
              "  27,\n",
              "  57,\n",
              "  93,\n",
              "  35,\n",
              "  71,\n",
              "  80,\n",
              "  31,\n",
              "  72,\n",
              "  31,\n",
              "  27,\n",
              "  64,\n",
              "  93,\n",
              "  62,\n",
              "  24,\n",
              "  70,\n",
              "  27,\n",
              "  93,\n",
              "  39,\n",
              "  27,\n",
              "  64,\n",
              "  15,\n",
              "  80,\n",
              "  80,\n",
              "  15,\n",
              "  51,\n",
              "  35,\n",
              "  27,\n",
              "  7,\n",
              "  26,\n",
              "  51,\n",
              "  24,\n",
              "  31,\n",
              "  70,\n",
              "  71,\n",
              "  31,\n",
              "  51,\n",
              "  62,\n",
              "  31,\n",
              "  90,\n",
              "  52,\n",
              "  90,\n",
              "  52,\n",
              "  53,\n",
              "  96,\n",
              "  72,\n",
              "  26,\n",
              "  93,\n",
              "  62,\n",
              "  79,\n",
              "  27],\n",
              " 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAfbKQmHphhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7729831-afab-41a0-94ea-439e4cc4366e"
      },
      "source": [
        "# shape data into final format\n",
        "import numpy as np \n",
        "\n",
        "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "  for j, char in enumerate(sequence):\n",
        "    X[i, j, char] = 1\n",
        "  y[i, next_char[i]] = 1\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((287846, 60, 107), (287846, 107))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb-Tz0Lwsz20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2d39b7b-7b28-441d-d475-ada6332526ee"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXCKegE-CDZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model imports \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc8e8ADis-9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "2fdc190e-ef76-4ae3-aae3-b7edca9e8d8b"
      },
      "source": [
        "# create model \n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer='adam', \n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               120832    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 107)               13803     \n",
            "=================================================================\n",
            "Total params: 134,635\n",
            "Trainable params: 134,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEGBTIttp4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "import random \n",
        "import sys \n",
        "\n",
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = data[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        # sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKdQSE8fuhN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0247dd75-cd31-48b7-b8a0-c5e05cbf45a6"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(X, y,\n",
        "          batch_size=1028,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 287846 samples\n",
            "Epoch 1/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.9166 - accuracy: 0.4430\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"int.\n",
            "  AUMERLE. Thou dar'st not, coward, live to see that d\"\n",
            "int.\n",
            "  AUMERLE. Thou dar'st not, coward, live to see that dround.\n",
            "  CEROID DON ILY Fras!\n",
            "\n",
            " Hene lortne;\n",
            "What so is neane dien'd. For'l now is andty, whece hericfur:\n",
            "    eake dorly the puct anat to ancouefay.\n",
            "  CAMETA.\n",
            "\n",
            "\n",
            " [_eriny bus itang her didtonch.\n",
            "\n",
            "\n",
            "\n",
            " BOVERE\n",
            "ON Cuce of, whan ith, and. GUUREs EDRVINT, OwLecA. SlEme foou, wy, Moftr, ghe praseen speno, grie,\n",
            "Yo\n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.9167 - accuracy: 0.4430\n",
            "Epoch 2/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8995 - accuracy: 0.4479\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"he make this way,\n",
            "    Under the colour of his usual game,\n",
            "\"\n",
            "he make this way,\n",
            "    Under the colour of his usual game,\n",
            "\n",
            "Catsead a the ingate whas hom hertenr, in. sowl sorns prong theeyâ.\n",
            "\n",
            "[_Brave, in fold.\n",
            "Theal spee duss the wrold tay, fir ad acf\n",
            "    BeAng thuth a im heme laum thiout hat\n",
            "    And so not se prawied thy rozenceny well tan eave\n",
            "    I he ranetor hed we dofrs wheas seoul her moke thus quaing,\n",
            "And bope with mounatt thre! hiy I mart\n",
            "    I \n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.8992 - accuracy: 0.4479\n",
            "Epoch 3/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8835 - accuracy: 0.4514\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"nk.\n",
            "\n",
            "\n",
            "      FRIAR.\n",
            "      To do what, signior?\n",
            "\n",
            "\n",
            "     \"\n",
            "nk.\n",
            "\n",
            "\n",
            "      FRIAR.\n",
            "      To do what, signior?\n",
            "\n",
            "\n",
            "                                TIUSA ENwanc: JANC, RWC_Krin, Say\n",
            "  TAYDAES. And you troogh.\n",
            "  BONANIHe sartutes, the come to dode you, or chall comats; far?\n",
            "  RORDHONDY. You my low, in thear HAndsecowio hame fursex'd;\n",
            "    In beang und your mess. Has, with Pathrounw ith my benosores, and no vage.\n",
            "\n",
            "HRY. That not know woll not or maer for;\n",
            "Thert hesh mar, stay to hise the grigh hime,\n",
            "Make to see \n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.8834 - accuracy: 0.4514\n",
            "Epoch 4/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8667 - accuracy: 0.4556\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"and for the liberal arts,\n",
            "Without a parallel: those being a\"\n",
            "and for the liberal arts,\n",
            "\n",
            "Wath yourd âe rastper well hasfucherker.\n",
            "\n",
            "\n",
            "O Heysturyrtore,\n",
            "For war ore of houter: betrest you malf is ceare\n",
            "Ande stronothess cateer? Permen thi'e in.\n",
            "Wer his with arin bofion wor thour cope,\n",
            "God tistated!\n",
            "\n",
            " BARIAL.\n",
            "\n",
            "  QUELES. Yow ail venkior; and you pureof or cogiond\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.8668 - accuracy: 0.4555\n",
            "Epoch 5/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8523 - accuracy: 0.4584\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"n\n",
            "After his studies or his usual pain?\n",
            "Then give me leave \"\n",
            "n\n",
            "After his studies or his usual pain?\n",
            "Then give me leave Lodes ginsung is he hil.\n",
            "A The rein ruspedst been to mighâd a live dot dear.\n",
            "\n",
            "    Which leve ama gorse the feilicr troust conge tleepâ to mus deow,\n",
            "\n",
            "  O I LuCt PeO, and Lorce; pirk-\n",
            "         \"  30\n",
            "  THIND. Sy domg. But be.\n",
            "  WASTOR. O goon have witha det hy\n",
            "   [Th o sean, with prome nance acd take ffor whet, I is,\n",
            "    Whe quese is these be ch\n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.8525 - accuracy: 0.4584\n",
            "Epoch 6/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8368 - accuracy: 0.4623\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"    A B C; to weep, like a young wench that had buried her g\"\n",
            "    A B C; to weep, like a young wench that had buried her gres;, dpouble bises?\n",
            "    And no be jundes nesour har'dst bearn, of bedy my fagcues mis ertior,\n",
            "Bet fill rear? Ang if!\n",
            "\n",
            "_Mleelien the sarmiatmon neetsedsard whatio,\n",
            "     lispy liow I bavy a but tiethât dist!\n",
            "    I' prall mam of thou sties ow gockes! Nomma\n",
            "    Is learâd you bunce, dut nom wills tonm toosert;\n",
            "'spats ane I aw thee Litca, the grittant of oub.\n",
            "\n",
            "DOMBUD.\n",
            "Cousnt on Coles wen\n",
            "287846/287846 [==============================] - 19s 64us/sample - loss: 1.8369 - accuracy: 0.4623\n",
            "Epoch 7/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8247 - accuracy: 0.4656\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"y, which is no grief to give.\n",
            "  GLOUCESTER. A greater gift \"\n",
            "y, which is no grief to give.\n",
            "\n",
            "    Thy undusherrce that or weach Loth, Crepirg. A Conithas,\n",
            "\n",
            "       if so hiliave it you hand the vorret.\n",
            "  USILDAf. I'r bo is with ill, bloosh of on, that be?\n",
            "\n",
            "\n",
            "    Bud your tomen this pad trou\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.8246 - accuracy: 0.4656\n",
            "Epoch 8/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8132 - accuracy: 0.4689\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"; they lack retention.\n",
            "Alas, their love may be called appet\"\n",
            "; they lack retention.\n",
            "Alas, their love may be called appetcent\n",
            "partlering to girest beat by tropp would eastles mane, is in hourseld,\n",
            "in oulr is nove joane faim, in my coust.\n",
            "\n",
            "AOTIANT.\n",
            "Iâlot covellostâs our wherield deater Beaagst,\n",
            "\n",
            "\n",
            "It uncestrek.\n",
            "\n",
            "\n",
            "I to, (ave ily?\n",
            "\n",
            "\n",
            "Entir forenterat and lave of that in will it in theâ\n",
            "Whan thou tray print Biy fitit\n",
            "287846/287846 [==============================] - 18s 63us/sample - loss: 1.8131 - accuracy: 0.4689\n",
            "Epoch 9/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.8027 - accuracy: 0.4715\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"ll?\n",
            "\n",
            "IACHIMO.\n",
            "Thanks, madam; well. Beseech you, sir,\n",
            "Des\"\n",
            "ll?\n",
            "\n",
            "IACHIMO.\n",
            "Thanks, madam; well. Beseech you, sir,\n",
            "\n",
            "\n",
            "A mise as froulss dost or nomen, of mike an the\n",
            "Obereâd friee you puy, sme haveringld,\n",
            "Even time your houlfâd, I pail hever he\n",
            "  cering love not hear, as moraty With'r Sake loun;\n",
            "The gade; pealfoner the rordpod me.\n",
            "\n",
            "Fould \n",
            "amas is dy Mastinim!\n",
            "Thene enmand\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 1.8028 - accuracy: 0.4715\n",
            "Epoch 10/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.7887 - accuracy: 0.4747\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"therefore, to revenge it, shalt thou die;\n",
            "    And so should\"\n",
            "therefore, to revenge it, shalt thou die;\n",
            "\n",
            "    That for slaer Freacuid herefunien lort\n",
            "\n",
            "\n",
            "    Sould you ponerstal to. Compor Goffeforâd,\n",
            "    For thene kingh will fell it to fe,\n",
            "    Is yousfouss wishtigsted shaw is bodre\n",
            "All wree, amens, And bestyel; fithole are the hair ablids; thot opest,\n",
            "\n",
            "\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 1.7888 - accuracy: 0.4746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f32c55f7e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvTa3cSoCIp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "746776a4-9150-4c92-dddb-b494a92da763"
      },
      "source": [
        "# create model V2\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 60, 128)           120832    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 107)               13803     \n",
            "=================================================================\n",
            "Total params: 266,219\n",
            "Trainable params: 266,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j95ooFH7C2eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fd444f6-25cb-485f-cedb-2c466b90c7f2"
      },
      "source": [
        "# fit model V2\n",
        "model.fit(X, y,\n",
        "          batch_size=1028,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 287846 samples\n",
            "Epoch 1/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 3.3107\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"ur life.\n",
            "Prince, pardon me, or strike me, if you please;\n",
            "I\"\n",
            "ur life.\n",
            "Prince, pardon me, or strike me, if you please;\n",
            "IL\n",
            "m\n",
            " e A Plbn n  ey   rehee ae\n",
            "   aiw  and Iluseula oatnaeg lt  booeeeyl saes?Ijmwuarhtylhnkge y.R\n",
            " HrTLF\n",
            "tIt eoom\n",
            " uoea\n",
            "OI.S\n",
            "\n",
            "m e ati' ton mt oiroame k bst\n",
            "287846/287846 [==============================] - 20s 70us/sample - loss: 3.3094\n",
            "Epoch 2/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.6543\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"ur deaths?\n",
            "    Who loves the King, and will embrace his par\"\n",
            "ur deaths?\n",
            "    Who loves the King, and will embrace his parrie tharesrevo doud feupe fikd;, witocoCd dirounoongoway urity thsoolotife sho Coonoer t ot whove andoyiljenoyemiuneiwnarghoulavad aun peais ariwaddeumcy woce berenhany cousdore inearothe areis yeintose, maikt eodâ\n",
            "   I Bimoin, teanos yatlerather iecsoare inle bouuâtsiecesuthord  oghibi eelwiâreiH oouthewenthitano oe yemevetomeyene wotighas, thee toero oogaoiceis apind â moo; cusonitodlmechhel\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 2.6535\n",
            "Epoch 3/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.3308\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"et my life be now as short,\n",
            "As my leave taking.  [Lies on t\"\n",
            "et my life be now as short,\n",
            "\n",
            " Fs heime touf calgett fad;\n",
            "H-op the. she hee mfind thertI\n",
            "r hdareSse dour humâdtige sulich tean\n",
            "     KM;SOLD. DI Dy yhiy. Yit, ar hy urot; sord Ande in sinerâ set; godse  at seinle an mod on.\n",
            "  AXO\n",
            "\n",
            "\n",
            "Yite.\n",
            "\n",
            "Asulin.\n",
            "LoLS. Kus time dues lonc,\n",
            "I tuands my ay mavele tet de sanzs ass. F\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 2.3307\n",
            "Epoch 4/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.2200\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \" shall remain with us.\n",
            "Now forth, Lord Constable and prince\"\n",
            " shall remain with us.\n",
            "Now forth, Lord Constable and prince!\n",
            "\n",
            " TARIG.,\n",
            "As wa ul? SA\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "AI\n",
            " o thy he Erlrit wite touâ\n",
            "\n",
            "\n",
            "'ARLheV FHE.\n",
            "Hhes thees theut at at of whil'\n",
            "\n",
            "FDERNURE.\n",
            "\n",
            " u she rule if iro, mu sisire sot inZ The shuarr upas harce,_ sCith\n",
            "\n",
            "PRHENTEL.\n",
            "Ter wouce\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 2.2200\n",
            "Epoch 5/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.1549\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"iving him glory.\n",
            "  THIRD CONSPIRATOR. Therefore, at your va\"\n",
            "iving him glory.\n",
            "  THIRD CONSPIRATOR. Therefore, at your vaver and yid gand\n",
            "Thith of alatin gormive, shask\n",
            "    minget wosbyave hot  oasiid Andussssendemes.\n",
            "\n",
            "\n",
            "Nouthikl sreosg, lashe resithy thon rortiste'd besarler oy hacther.\n",
            "\n",
            "TYIYISTI;.\n",
            "\n",
            "\n",
            "LoPtOsGo'ghbeon pby coling ghenanks, wind bot ronk-ratorcurintfest.\n",
            "J They forthegkes thoy \n",
            "287846/287846 [==============================] - 19s 66us/sample - loss: 2.1549\n",
            "Epoch 6/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.1066\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"y\n",
            "Oâerpays all I can do. By this your king\n",
            "Hath heard of\"\n",
            "y\n",
            "Oâerpays all I can do. By this your king\n",
            "Hath heard of nis to Bus hert\n",
            "\n",
            "   AHATAUH.\n",
            "\n",
            "\n",
            "\n",
            "    AR ILKERO\n",
            "\n",
            "\n",
            "   VIULDE.\n",
            "Yor, sor sA?\n",
            "\n",
            "GALL.\n",
            "Mepsunsnder som cinstan thoid fade is of an wrus b\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 2.1064\n",
            "Epoch 7/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.0663\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"gh at that.âPrythee,\n",
            "Horatio, tell me one thing.\n",
            "\n",
            "HORAT\"\n",
            "gh at that.âPrythee,\n",
            "Horatio, tell me one thing.\n",
            "\n",
            "HORATINE,Fe. Is a daidh meite if, the ticher wome wighe wharouth tpite hill\n",
            "    Hithy ve outh you ghie beties for,\n",
            "  f mave athay proud teen efee he ditgigh,\n",
            "\n",
            "\n",
            "SIO I TELTH.\n",
            "\n",
            "\n",
            "\n",
            "MADCONY.\n",
            "Aafred he ane amainder tit ermofalas.\n",
            "   Cpeter theadors coich you urame sit\n",
            "  ELTIPU Keres; har,\n",
            "would, yo\n",
            "287846/287846 [==============================] - 18s 64us/sample - loss: 2.0658\n",
            "Epoch 8/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.0321\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"    And, as I hear, the great commanding Warwick\n",
            "    Is thi\"\n",
            "    And, as I hear, the great commanding Warwick\n",
            "\n",
            "  TIRG  EMtHEROUUS a woud nout.\n",
            "\n",
            "\n",
            "\n",
            "  And as mate, braod, the leeith and tut cupterser qooods-\n",
            "Thave dy weld eatitit., hees hatearmy.\n",
            "\n",
            "\n",
            "\n",
            "ARLONT. Ithin Cartening rer's toue seveny my fer,\n",
            "    of I stim!\n",
            "\n",
            "\n",
            "\n",
            "RENBE.\n",
            "\n",
            "Andtine of poist\n",
            "    Pey u merenoss soep\n",
            "287846/287846 [==============================] - 19s 64us/sample - loss: 2.0321\n",
            "Epoch 9/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 2.0036\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \" not whoop at them;\n",
            "But thou, âgainst all proportion, did\"\n",
            " not whoop at them;\n",
            "\n",
            "    Rhaf. Mhay bewith to chave, ard teereâsâg.\n",
            "\n",
            "    Athiest ssy you will lety popkatquoos fadtt, witot\n",
            "    ARchithof pia, shad pulligter.\n",
            "\n",
            "GARNUNTHER. As tode?â LondâD, and bapcomart, loslidt and gland,\n",
            "Thous thou wam dcole knontale dather badeg.\n",
            "  NSENUNEO]  What\n",
            "287846/287846 [==============================] - 19s 66us/sample - loss: 2.0036\n",
            "Epoch 10/10\n",
            "286812/287846 [============================>.] - ETA: 0s - loss: 1.9743\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \".\n",
            "  SPEED. For me?\n",
            "  LAUNCE. For thee! ay, who art thou? H\"\n",
            ".\n",
            "  SPEED. For me?\n",
            "\n",
            "\n",
            "   Tout  thiy s ciskentar. O De Derededt INR IUSsay\n",
            "LReRceA KEreY GodeRâ, Korulis..\n",
            "\n",
            "\n",
            "\n",
            "LoR. CUTHERR. a lainerse, ther JrercenR, Thould of ane so he; ichy you tith.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ENERLEN. Thenk, I minchendens you dow hor-. Whrast to thoresse fure were;\n",
            "    Whe. fave mumane't Sa\n",
            "287846/287846 [==============================] - 19s 65us/sample - loss: 1.9741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f00e5a2f908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb99jppFDbMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "cba2bca5-358c-4b69-a44b-a937e146e093"
      },
      "source": [
        "# create model V3\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)), recurrent_dropout=.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 128)               120832    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 107)               13803     \n",
            "=================================================================\n",
            "Total params: 134,635\n",
            "Trainable params: 134,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwtsWrJLD8dq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c09279c0-7815-4dda-8ffa-00b0cdd825e3"
      },
      "source": [
        "model.fit(X, y, batch_size=514, epochs=10, callbacks=[print_callback])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 287846 samples\n",
            "Epoch 1/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 2.9573\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"emniz'd.\n",
            "    Is not the Lady Constance in this troop?\n",
            "    \"\n",
            "emniz'd.\n",
            "    Is not the Lady Constance in this troop?\n",
            "    ;      T me\n",
            " Ih con ilwr  wo ?\n",
            "\n",
            "   N  as  anâ\n",
            " H f  y ait Wate y Wh, cheomee th, rhebe heris lla'thesu, idhfrmiel's hedwrlame wht inthaobenarl, Toxlhiater, f:lin cath wan,twi cones cl?therrxI s. aud, indthisUpe; bto se aor 2ofe boso.d;\n",
            "AAUMSOIDONPAHc.\n",
            "\n",
            " ArH-,\n",
            "Ane  e c, fhamal surâsl heten hinor n?\n",
            "Thoe  icrofche,  o tas yorid taI;\n",
            "ARHIEGoNnd\n",
            "287846/287846 [==============================] - 60s 208us/sample - loss: 2.9573\n",
            "Epoch 2/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 2.3487\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"ing, fiery-red with haste.\n",
            "  BOLINGBROKE. Welcome, my lords\"\n",
            "ing, fiery-red with haste.\n",
            "  BOLINGBROKE. Welcome, my lordse mamnis and te blawern.\n",
            "   AnT HEMBES.\n",
            "Som oflre tha met sker possradrerrelr youwivis]giving\n",
            "    MFon wbisce oonthir toatw llilew, Tomeldenghansounde jopsy.\n",
            "\n",
            "e urw\n",
            "\n",
            "AgILNOSL.\n",
            "A tho hevt?\n",
            "ANI Somoou lisx. aBlaseer Is wralviy snovel.\n",
            "Crlaf py youTt or.\n",
            "\n",
            "TFoupRdAROCLINR. E ISRWOOS. W\n",
            "287846/287846 [==============================] - 60s 210us/sample - loss: 2.3487\n",
            "Epoch 3/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 2.1898\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"\n",
            "      I would rather have one of your fatherâs getting. H\"\n",
            "\n",
            "\n",
            "\n",
            "Yoort yof lot naCECVRTATs.\n",
            "Aot gruy lise and in pie.\n",
            "\n",
            "\n",
            "xed bingl\n",
            "My, in leosu'd incl ond vestuth this hea botk, farr.\n",
            "\n",
            "DERLOUW.  Tore praichsâs por.\n",
            "ERSANLEGSMe gor.\n",
            "\n",
            "\n",
            "SORESE.\n",
            "âh thou din ige mes thiis garr ung the thas an  yow he hade hime thit wimen is the thee thin, are\n",
            "\n",
            "  Sencenw, and wot be you hith go pae she win. not snmungh non \n",
            "287846/287846 [==============================] - 60s 208us/sample - loss: 2.1898\n",
            "Epoch 4/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 2.1042\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"air,\n",
            "Or if it were it bore not beautyâs name:\n",
            "But now is\"\n",
            "air,\n",
            "Or if it were it bore not beautyâs name:\n",
            "But now is priond ta âsinot so jule fosiarghrin.\n",
            "    Noan ahir, wim o mead and tuthobdeser beaothem. Whot whor hila fitas more;\n",
            "Thel anchit?\n",
            "\n",
            "CEENUE.\n",
            "                                   1Tabe, I a lly perae,\n",
            "Thou qurlertartfer worwelot'y,\n",
            "Fie wane, OToutay, fo thor becojeraceafe vere horspenot bet\n",
            "       Ender babre teeby mart [ongathert bat dey, to thur onchet un and arec'scningw, I\"silyeen\n",
            "s a l\n",
            "287846/287846 [==============================] - 60s 210us/sample - loss: 2.1041\n",
            "Epoch 5/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 2.0444\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"ould offend him; and in his offence\n",
            "    Should my performan\"\n",
            "ould offend him; and in his offence\n",
            "    Should my performan is is favese\n",
            "Preitel the issert, you ermabee.\n",
            "\n",
            "    Low waly mowe the yeallald nen sear;\n",
            "\n",
            "\n",
            "\n",
            "Afrist pruce. I Rowlly the lithur in bes os memt as min yous.     Mat you mppeikg on thour the the toigure the .\n",
            "  CLEROR. SBy. Heth your my hou mutcreaysling. Antâ ar ant has wow armantest?\n",
            "\n",
            "ESOSGEE.\n",
            "287846/287846 [==============================] - 61s 211us/sample - loss: 2.0444\n",
            "Epoch 6/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 1.9997\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"ir Valentine and servant, to you two thousand.\n",
            "  SPEED.  [A\"\n",
            "ir Valentine and servant, to you two thousand.\n",
            "  SPEED.  [ATy for had\n",
            "And cands!\n",
            "\n",
            "CIAC. En, of ghat thing lene, and sastrel wick peag,\n",
            " Taie, for lavish cofringer shous.\n",
            "\n",
            "  TRARCS.\n",
            "Kith'r dith 'ster-âAse, hit sur id Of ajout;.\n",
            "\n",
            "\n",
            "O thy, theer, corde,\n",
            "    od einger LegreneehtA, ne dena,lance\n",
            "s   oid a no the deom?\n",
            "\n",
            "LOESF INM.\n",
            "[Kigptor hillat, whour cound your tool\n",
            "287846/287846 [==============================] - 60s 210us/sample - loss: 1.9997\n",
            "Epoch 7/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 1.9635\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"ff\n",
            "    To win a vulgar station; our veil'd dames\n",
            "    Commi\"\n",
            "ff\n",
            "    To win a vulgar station; our veil'd dames\n",
            "    Commicover bomdwat his dearn youle didnels.\n",
            "Shave- sirst I  is lord wher watht weve ford. Boraveress, tanert.\n",
            "  FASSAOFSNDR. Lime blmest dove be thou make ingnefs\n",
            "    And if the blouthy. my and eas in hasveed ptay his far dy, the wo hat Himing frowus?\n",
            "\n",
            "PRYOUTEWIm.\n",
            "Myhe that, acrate Toost verse;\n",
            "    \n",
            "Far hat lother fored hand to beth blots, quebon.\n",
            "\n",
            "KILGAR.\n",
            "Thy shall lowcen'd fight hidgit,\n",
            " \n",
            "287846/287846 [==============================] - 60s 209us/sample - loss: 1.9635\n",
            "Epoch 8/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 1.9327\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"he kite builds, look to lesser linen. My\n",
            "father named me Au\"\n",
            "he kite builds, look to lesser linen. My\n",
            "father named me Auner wallyây:\n",
            "Nove to meke we redy lay'de'ds:\n",
            "Thinst wones woyed thith the Juld,, I Alathiswere\n",
            "    hun Mansen; of stheps. Yanige, thank kncentertâs me as ensuây thy Downy\n",
            "\n",
            "    Lourdedss on the have woaldy browcere worsing\n",
            "And thou frightrod?\n",
            "\n",
            " [_Enceat, Awila;âd actornook:\n",
            "WilG'd he doblimy Doth savent; you came de to proce.\n",
            "  ARCAN\n",
            "287846/287846 [==============================] - 60s 209us/sample - loss: 1.9327\n",
            "Epoch 9/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 1.9050\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"nal;\n",
            "    For, being not mad, but sensible of grief,\n",
            "    My\"\n",
            "nal;\n",
            "    For, being not mad, but sensible of grief,\n",
            "    Myse the bisinionce the me my may I knom ofran.\n",
            "    Inone propin ar wit survere the vere to thay\n",
            "\n",
            "\n",
            "\n",
            "SCACTIND.\n",
            "Mind sirur they, that yous.\n",
            "    on the fares'll with, mane it his\n",
            "    The wosle yours nough? Thereâs, cources buter,\n",
            "\n",
            "\n",
            "BYOSSEL.\n",
            "Ant nease, word the Ka\n",
            "287846/287846 [==============================] - 61s 212us/sample - loss: 1.9050\n",
            "Epoch 10/10\n",
            "287840/287846 [============================>.] - ETA: 0s - loss: 1.8797\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"head broke.\n",
            "\n",
            "BAPTISTA.\n",
            "How now, my friend! Why dost thou \"\n",
            "head broke.\n",
            "\n",
            "BAPTISTA.\n",
            "How now, my friend! Why dost thou doone her goad;\n",
            "That unouersuges of that noorned; but my ther him ow\n",
            "\n",
            "    as heave with shary vorer and be nold?\n",
            "\n",
            "HADSUS.\n",
            "Aosllour his keaved aur hinglantt thee utter:\n",
            "    Ot can is a thoo me, browd! Lothim.\n",
            "    Mill benvion do firta arigesl; and to to wheme,\n",
            "    Andlls coom; my bey silive of ale youn?\n",
            "in here well are sur.       I   for have \n",
            "287846/287846 [==============================] - 61s 211us/sample - loss: 1.8797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0094a6f898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}